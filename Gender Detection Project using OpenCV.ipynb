{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d8e445",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5735b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df671c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceBox(faceNet,frame):\n",
    "    # On 0 no was it height.\n",
    "    frameHeight=frame.shape[0]\n",
    "    # on 1 no was it width\n",
    "    frameWidth=frame.shape[1]\n",
    "    # cv2.dnn.blobFromImage(image,scalefctor=1.0,size,mean,swapRB=True/False).\n",
    "    # blob will create in 4 dimensions we put here our frame scale vector=1.0,metrics=(300,300),mean values=[104,117,123].\n",
    "    blob=cv2.dnn.blobFromImage(frame, 1.0, (300,300), [104,117,123], swapRB=False)\n",
    "    # It means our blob is created and we converted them in 4 dimensions\n",
    "\n",
    "    \n",
    "    # It means facenet is our algorithum and we SetInput here our blob.\n",
    "    faceNet.setInput(blob)\n",
    "    # Then it will do detection and our detection came in 4 dimensions.Our 1st element in 1st dimension,2nd element in 2nd dimension,3rd element in 3rd dimension and 4th element in 4th dimensions.\n",
    "    # our detection will came.\n",
    "    detection=faceNet.forward()\n",
    "    # So all the faces it get in the blob so for faces obses we take the blank list and it will read the faces as much as it have.\n",
    "    bboxs=[]\n",
    "    \n",
    "    #so in this we give our detection and we put our 2nd dimension 1st and 2nd dimensions put our flairs,here so all the face of the points are in 3rd dimension and 4th dimensions will take our channels\n",
    "    # Then our i values will came so at too it around 127 value will made.\n",
    "    for i in range(detection.shape[2]):\n",
    "        \n",
    "        # we give in detection[0,0(this is our first two element),i,2]\n",
    "        confidence=detection[0,0,i,2]\n",
    "        \n",
    "        # if our confidence if greater than our thresh_hold value(0.7) so our confidence value came above 70% so we draw our rectangle(How this rectangle was made x1,y1,x2,y2) (47.00)\n",
    "        # our starting is our x1 and y1 and where it ends is our x2 and y2.We know that in x=width and y=height.\n",
    "        if confidence>0.7:\n",
    "            # In x1 and x2 where we multiply with frameWidth.\n",
    "            x1=int(detection[0,0,i,3]*frameWidth)\n",
    "            # In y1 and y2 where we multply with FrameHeight.\n",
    "            y1=int(detection[0,0,i,4]*frameHeight)\n",
    "             # In x1 and x2 where we multiply with frameWidth.\n",
    "            x2=int(detection[0,0,i,5]*frameWidth)\n",
    "             # In y1 and y2 where we multply with FrameHeight.\n",
    "            y2=int(detection[0,0,i,6]*frameHeight)\n",
    "            # Then our box is created and it points will came x1 and y1 came.so all the width and height acc to that our x1 and y1 value will came same our x2 and y2 will came.\n",
    "            \n",
    "            \n",
    "            # so we put them all [x1,y1,x2,y2]in list and we append them so our one face dimension came.\n",
    "            bboxs.append([x1,y1,x2,y2])\n",
    "            \n",
    "            # Then we create a rectangle our image(frame,(x1,y1),(x2,y2),color,linwidth)\n",
    "            cv2.rectangle(frame, (x1,y1),(x2,y2),(0,255,0), 1)\n",
    "            \n",
    "            # so we return here our frame and bboxs.\n",
    "    return frame, bboxs\n",
    "\n",
    "\n",
    "# After then we load our 6 files.\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    " \n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "# Then we create an face net which is used to read face cv2.dnn(d neural network).readNet we used this method it will read each and every thing if we want to read face so we give here facemodel and faceproto we put them in faceNet in (faceModel we have neuralNetworks) and in (faceproto we have our weights.)\n",
    "# so we read the face dataset of our model and we store in faceNet.\n",
    "faceNet=cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "# same we do with ageNet.We store in ageNet.\n",
    "ageNet=cv2.dnn.readNet(ageModel,ageProto)\n",
    "\n",
    "# same we do with gendernet we save in genderNet\n",
    "genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
    "\n",
    "# So all the networks means all the layers of network and all the weights of our model we read our all weights and store them in faceNet,ageNet and genderNet.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then we set mean value of our model suppose this is our face (32.00) This face have mean value and this is off RGB.we take this value high as well low.But when we take this values high as well as low so our prediction will be worst.\n",
    "# we have to take mean values by our side\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "\n",
    "# Then we create our ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)'].\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "\n",
    "# Then we take genderlist=['Male', 'Female'] with this thing we have to do prediction.\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "\n",
    "\n",
    "# Then we do video=cv2.VideoCapture(0)\n",
    "video=cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Then we give padding=20()\n",
    "padding=20\n",
    "\n",
    "while True:\n",
    "    # it means we give here our ret=true/false and frame which means our metrics.\n",
    "    ret,frame=video.read()\n",
    "    # Then we resize our frame and we store it in frame.\n",
    "    frame=cv2.resize(frame,(600,600))\n",
    "    # We give two things in faceBox one is faceNet(which read face) and other is frame which is our metrics.facenets have all the networks layer and models so it will read all the networks and we give frame of our metrics.\n",
    "    frame,bboxs=faceBox(faceNet,frame)\n",
    "    \n",
    "    \n",
    "# let's understand how DNN works we will do work with our face suppose we have three face(50.32) so we break our imag acc to pixels.see we have the pixels metrics which is formed where we have the each and every box.How we read them .like we study haarclassifier first they read 1st one,then they read 2nd one,then they read 3rd one if it get points for the face then it get's stop otherwise it will continuse.If it get so it create an rectangle so where we get the points o it is created.So one we have the face here and other here so our rectangle is made so we create x1 and y1 from DNN\n",
    "    \n",
    "    \n",
    "    \n",
    "    # If it read the box so it came for bbox in bboxs.means in face bixes two facebox came so we take only one face bbox.\n",
    "    for bbox in bboxs:\n",
    "        # we get two points of frame one is x-axis and other is y-axis.we get it maximum argument of 0 and facebox[1].It have four things x1,y1,x2,y2 so facebox[1] means y1 so with that we minus value of padding so our argument will came.Padding was just a value we subtract them why we subtract them we create a rectangle so we want to detect it face from age and gender so we crop our face from facebox[3] it means y2 in this we add padding upto 20.\n",
    "        # so this was our height and  so this was our width.\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        \n",
    "        # Now our face will came to us.Now we want to detect our face so we want to create blob for our face earlier we detect face from Image.we create blob for image.\n",
    "        # from face we have to detect gender and age.\n",
    "        \n",
    "        # so we are creating blob for face(face,size,mean values,swap_RB=False) so our blob is created now we want to predict gender.\n",
    "        blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        \n",
    "        # We just setInput our so all the process will run like weight,neural network etc\n",
    "        genderNet.setInput(blob)\n",
    "        # Our predictions came from genderNet.forward().So it will predict gender .ALl the value in gender prediction  we have labels male and female we take the list so we do genderprediction in 0 no index.\n",
    "        # so it value came b/w 0 and 1.If we have 0 value came then it is Male and If we have 1 value came then it is female.\n",
    "        genderPred=genderNet.forward()\n",
    "        gender=genderList[genderPred[0].argmax()]\n",
    "\n",
    "\n",
    "        # Same we do work with ageNet like we did with genderNet.\n",
    "        ageNet.setInput(blob)\n",
    "        agePred=ageNet.forward()\n",
    "        age=ageList[agePred[0].argmax()]\n",
    "\n",
    "\n",
    "        # we already predicted our gender and age so our values came\n",
    "        \n",
    "        \n",
    "        \n",
    "        label=\"{},{}\".format(gender,age)\n",
    "        \n",
    "        # Then we draw our rectangle.We give here our frame.-doubt\n",
    "        cv2.rectangle(frame,(bbox[0], bbox[1]-30), (bbox[2], bbox[1]), (0,255,0),-1) \n",
    "        # Then we put a text here.we give here frame,label,x1=bbox[0],y1=bbox[1],font_name,scalefactor,colorsize,size,linetype.\n",
    "        cv2.putText(frame, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2,cv2.LINE_AA)\n",
    "        # Then we show our frame.\n",
    "    cv2.imshow(\"Age-Gender\",frame)\n",
    "    # Then we give here our Waitkey.\n",
    "    if cv2.waitKey(1) & 0xff==ord('q'):\n",
    "        break\n",
    "\n",
    "# Then we release our video\n",
    "video.release()\n",
    "# Then we destroyAllWindows().\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90c903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
